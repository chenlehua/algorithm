# 推荐系统特征表示演进：从人工到智能的范式跃迁

> **核心命题**：推荐/CTR模型的演进本质上是一部**特征表示的自动化与智能化史**——从人工定义的离散符号，走向模型自主学习的连续语义向量。

---

## 📚 知识框架速览

```
特征表示演进 = 自动化程度↑ + 表示能力↑ + 端到端程度↑

┌─────────────────────────────────────────────────────────────────┐
│  人工特征 → 树模型自动化 → Embedding革命 → 混合智能架构        │
│  (离散稀疏)   (组合自动化)   (连续稠密)      (显式+隐式统一)    │
└─────────────────────────────────────────────────────────────────┘
```

---

## 🎯 四阶段演进详解

### 第一阶段：人工特征工程时代（~2014）

#### 核心模型：逻辑回归（LR）

**数学本质**：
$$y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)$$

其中 $\sigma$ 为sigmoid函数，模型仅能学习**线性叠加关系**。

#### 核心矛盾

| 现实需求 | 模型能力 | 解决方案 |
|---------|---------|---------|
| 组合效应（如"年轻+电竞鼠标"） | 仅支持线性叠加 | 人工构造交叉特征 |
| 特征交叉 `age=young & category=gaming_mouse` | 无法自动发现 | 业务专家经验驱动 |

#### 三重困境（记忆口诀：**爆·稀·滞**）

| 困境 | 问题描述 | 量化表现 |
|-----|---------|---------|
| **组合爆炸** | n个特征的k阶组合呈指数增长 | 二阶: $O(n^2)$，三阶: $O(n^3)$ |
| **数据稀疏** | 长尾组合几乎无训练样本 | 大量交叉特征权重为0 |
| **经验依赖** | 特征设计滞后于业务变化 | 迭代周期长，人力成本高 |

#### 典型工作流

```
原始特征 → 特征工程师手工设计 → 交叉特征 → LR训练 → 上线
    ↑                                              |
    └──────────── 效果反馈，人工迭代 ←─────────────┘
```

---

### 第二阶段：树模型的自动化尝试（2014~2016）

#### 代表方案：Facebook GBDT+LR（2014）

**核心思想**：用GBDT的决策路径自动编码特征组合

#### GBDT原理详解（图解版）

**GBDT = Gradient Boosting Decision Tree（梯度提升决策树）**

**一句话核心**：串行训练多棵树，每棵树专门学习"前面的树哪里预测错了"，最后把所有树的预测加起来。

#### 🎯 比喻：射箭修正

```
目标：射中靶心（10环）

第1箭：射到6环（偏左下）
       教练："往右上修正"
       
第2箭：专门往右上调 → 修正了3环 → 现在6+3=9环
       教练："再往右上一点"
       
第3箭：继续微调 → 修正0.8环 → 现在9.8环

第4箭：再调 → 修正0.15环 → 9.95环 ≈ 靶心！
```

**对应关系**：
- 每棵树 = 一次射箭
- 残差 = 偏离靶心的距离
- 下一棵树 = 专门修正上一次的偏差

#### 📝 比喻：学生改错题

```
目标：考100分

【第一轮】考了60分，错了3道题
【第二轮】只学错题 → 搞懂2道 → 累计80分
【第三轮】继续攻克剩余错题 → 累计93分
【第四轮】... → 逐渐逼近100分

关键：每轮只关注"还没学好的部分"，不断查漏补缺
```

#### GBDT vs 随机森林

| | 随机森林 | GBDT |
|--|---------|------|
| **比喻** | 10人独立猜年龄，取平均 | 第1人猜，第2人猜"偏了多少"，不断修正 |
| **树的关系** | 互不相干 | 后面纠正前面的错误 |
| **策略** | 人多平均更准 | 不断改进越来越准 |

#### 具体示例

```
预测用户年龄，真实值 = 25岁

树1预测：20岁 → 偏差5岁
树2修正：+3岁 → 累计23岁 → 还差2岁
树3修正：+1.5岁 → 累计24.5岁 → 还差0.5岁
树4修正：+0.4岁 → 累计24.9岁 → 几乎准确！

最终预测 = 树1 + 树2 + 树3 + 树4 = 20+3+1.5+0.4 = 24.9
```

> **"梯度"** = 偏差的方向（告诉下一棵树往哪修正）
> **"提升"** = 每加一棵树，预测就更准一点

#### GBDT+LR 的工作流程

```
原始特征 → GBDT训练 → 每棵树的叶节点路径 → One-Hot编码 → LR
```

**关键洞察**：GBDT每条从根到叶的路径，天然编码了一组特征判断的**逻辑合取**（AND关系）

**详细示例**：

```
假设训练了2棵树，每棵树有4个叶节点

树1结构：                        树2结构：
       [age<25?]                      [income>5000?]
       /       \                       /          \
   [是]        [否]              [gender=M?]    [叶4]
   /  \        /   \              /      \
[叶1] [叶2] [叶3] [叶4]        [叶1]    [叶2]    [叶3]

对于用户 x = {age=22, income=8000, gender=M}:

树1路径: age<25 → 是 → 叶1      编码: [1,0,0,0]
树2路径: income>5000 → gender=M → 叶1  编码: [1,0,0,0]

拼接后的特征向量: [1,0,0,0, 1,0,0,0]
                  ←─树1─→  ←─树2─→

这个向量输入LR进行最终预测
```

**为什么有效？**
- 叶节点编码 = 一组特征条件的合取（如 `age<25 AND income>5000 AND gender=M`）
- GBDT自动发现有区分度的特征组合
- 相当于**自动化的特征工程**

#### 进步与局限对比

| 维度 | 进步 ✓ | 局限 ✗ |
|-----|--------|--------|
| 自动化 | 组合特征自动发现 | 两阶段训练，非端到端 |
| 表示能力 | 隐式学习高阶交叉 | 离散表示缺乏语义泛化 |
| 工程效率 | 减少人工特征设计 | GBDT更新成本高 |

#### 本质理解

> GBDT+LR 是**"用结构化模型替代人工做特征工程"**的第一次成功尝试，但仍停留在**离散表示**范式内。

---

### 第三阶段：Embedding与深度网络的范式革命（2016~2017）

#### 关键突破：从离散到连续

| 传统方式 | Embedding方式 |
|---------|--------------|
| One-Hot: [0,0,1,0,...,0] (百万维) | Dense Vector: [0.23, -0.45, ..., 0.12] (128维) |
| 稀疏、高维、无语义 | 稠密、低维、语义化 |

#### Embedding的三重价值

1. **降维压缩**：从百万维→百维，存储/计算效率提升万倍
2. **语义化**：向量空间中的距离和方向承载抽象概念
3. **泛化能力**：相似实体向量相近，未见组合也可通过向量运算推理

```
Embedding空间示意：
                    
     电竞鼠标 ●────● 机械键盘
              \  /
               \/
               ● 游戏耳机
              
    ● 婴儿奶粉 ────● 纸尿裤
```

#### 深度网络：通用函数逼近器

**MLP的理论能力**：
$$f(x) = W_L \cdot \sigma(W_{L-1} \cdot \sigma(\cdots \sigma(W_1 \cdot x)))$$

可逼近任意连续函数，**理论上能隐式建模任意阶特征交互**。

**实践局限**：
- 全连接结构缺乏先验引导
- 学习简单二阶交叉需要大量参数
- "大炮打蚊子"——低效且不可解释

#### 代表模型

| 模型 | 年份 | 核心创新 |
|-----|------|---------|
| Deep Crossing | 2016 | 首次将深度学习引入CTR |
| YouTube DNN | 2016 | 召回+排序双塔架构 |
| PNN | 2016 | 显式Product层捕捉交互 |

---

### 第四阶段：混合智能架构的成熟（2016~至今）

#### 设计哲学

> **记忆（Memorization）** + **泛化（Generalization）** 在同一框架内协同工作

| 能力 | 含义 | 对应组件 |
|-----|------|---------|
| 记忆 | 记住历史共现模式 | Wide/显式部分 |
| 泛化 | 推理未见过的组合 | Deep/隐式部分 |

#### 深入理解"记忆"与"泛化"

**记忆（Memorization）** = 直接记住训练数据中**反复出现的共现规则**，不需要理解"为什么"。

**具体例子**：

假设训练数据中有这样的模式：
```
历史数据显示：
  • 安装了"王者荣耀"的用户 → 80%会点击"游戏手柄"广告
  • 购买过"奶粉"的用户 → 75%会点击"纸尿裤"广告
```

**Wide部分的做法**：
```
交叉特征: [installed_app=王者荣耀 & ad_category=游戏手柄] → 权重 w = 2.5 (高)
交叉特征: [purchased=奶粉 & ad_category=纸尿裤] → 权重 w = 2.3 (高)
```

它直接给这些**高频共现组合**赋予高权重，相当于"背下来"了这些规则。

**记忆 vs 泛化 对比**：

| 场景 | Wide/记忆 | Deep/泛化 |
|-----|----------|----------|
| 见过的高频组合 | ✅ 直接查表，效果好 | ✅ 也能学到 |
| **没见过的新组合** | ❌ 权重为0，无法预测 | ✅ 通过Embedding相似性推理 |

**例子**：
```
新游戏"原神"上线，没有历史数据

Wide: [installed_app=原神 & ad_category=游戏手柄] → 权重=0（没见过）
Deep: "原神"的Embedding与"王者荣耀"相近 → 可推理出也可能喜欢游戏手柄
```

**一句话理解**：
> **记忆** = 背答案（见过的直接查表）
> **泛化** = 学方法（没见过的也能推理）

Wide & Deep 的设计哲学：**背答案处理高频模式，学方法处理长尾需求**，两者互补。

#### 代表模型对比

| 模型 | Wide/显式部分 | Deep/隐式部分 | 关键贡献 |
|-----|--------------|--------------|---------|
| **Wide & Deep** (2016) | 线性模型+手工交叉 | MLP | 首提记忆-泛化框架 |
| **DeepFM** (2017) | FM（自动二阶交叉） | MLP | 端到端统一，无需人工特征 |
| **DCN** (2017) | Cross Network | MLP | 显式高阶交叉，参数高效 |
| **xDeepFM** (2018) | CIN | MLP | 向量级显式交互 |

#### DeepFM 深度解析

**架构图**：
```
         ┌─────────────────────────────────────┐
         │            Output Layer             │
         └───────────────┬─────────────────────┘
                         │
            ┌────────────┴────────────┐
            │                         │
     ┌──────┴──────┐          ┌───────┴───────┐
     │  FM Output  │          │  DNN Output   │
     │   (二阶)    │          │   (高阶)      │
     └──────┬──────┘          └───────┬───────┘
            │                         │
     ┌──────┴──────┐          ┌───────┴───────┐
     │ FM Layer    │          │ Hidden Layers │
     │ <vi,vj>     │          │    (MLP)      │
     └──────┬──────┘          └───────┬───────┘
            │                         │
            └────────────┬────────────┘
                         │
              ┌──────────┴──────────┐
              │   Embedding Layer   │  ← 共享！
              │  (特征→稠密向量)     │
              └──────────┬──────────┘
                         │
              ┌──────────┴──────────┐
              │    Input Layer      │
              │  (Sparse Features)  │
              └─────────────────────┘
```

**FM组件的数学优雅**：

原始二阶交叉复杂度 $O(n^2)$：
$$\sum_{i=1}^{n}\sum_{j=i+1}^{n} w_{ij} x_i x_j$$

FM通过矩阵分解降至 $O(kn)$：
$$\sum_{i=1}^{n}\sum_{j=i+1}^{n} \langle v_i, v_j \rangle x_i x_j = \frac{1}{2}\left[\left(\sum_{i=1}^{n} v_i x_i\right)^2 - \sum_{i=1}^{n} v_i^2 x_i^2\right]$$

**核心优势**：
- ✓ 自动学习所有二阶特征交叉
- ✓ FM与DNN共享Embedding，端到端训练
- ✓ 无需任何人工特征工程

---

## 📊 演进主线总结表

| 阶段 | 时期 | 特征表示形态 | 交叉方式 | 核心突破 | 代表模型 |
|-----|------|-------------|---------|---------|---------|
| **人工时代** | ~2014 | 离散·稀疏·符号化 | 人工设计 | — | LR |
| **树模型** | 2014~2016 | 离散·稀疏·自动生成 | 决策路径编码 | 组合自动化 | GBDT+LR |
| **深度学习** | 2016~2017 | 连续·稠密·可计算 | MLP隐式学习 | 语义化表示 | Deep Crossing |
| **混合架构** | 2016~至今 | 连续·稠密·结构引导 | 显式+隐式并行 | 端到端统一 | DeepFM, DCN |

---

## 🧠 深层洞察

### 终极目标

> 构建一个连接**用户、物品与上下文**的**"通用语义空间"**

### 语义空间的三大能力

| 能力 | 实现方式 | 应用场景 |
|-----|---------|---------|
| **相似性可度量** | 余弦距离、欧氏距离 | 相似物品召回 |
| **关系可推理** | 向量运算≈语义运算 | 知识图谱嵌入 |
| **组合效应可表达** | 神经网络非线性变换 | 特征交叉建模 |

### 后续发展方向

```
DeepFM (2017) 
    ↓
注意力机制增强
    ↓
Transformer推荐模型 (2018~)
├── SASRec: 自注意力序列推荐
├── BERT4Rec: 双向编码推荐
└── 更动态、更灵活地捕捉序列上下文中的特征交互
```

> **本质不变**：始终在"表示学习"框架下，追求更好的特征语言构建能力

---

## 🎓 记忆口诀

### 四阶段口诀

```
人工交叉累成狗（LR时代）
树模型来帮帮手（GBDT+LR）
Embedding语义有（Deep Learning）
显隐结合全都有（DeepFM）
```

### 核心概念口诀

```
特征表示三维度：稀疏稠密、离散连续、人工自动
混合架构两能力：记忆靠Wide、泛化靠Deep
FM优雅在哪里：向量内积、复杂度低、全自动化
```

---

## 🖼️ 知识全景图

```
┌────────────────────────────────────────────────────────────────────────────┐
│                    推荐系统特征表示演进全景图                                │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐ │
│  │  第一阶段   │───→│  第二阶段   │───→│  第三阶段   │───→│  第四阶段   │ │
│  │  人工特征   │    │   树模型    │    │ 深度学习    │    │  混合架构   │ │
│  └──────┬──────┘    └──────┬──────┘    └──────┬──────┘    └──────┬──────┘ │
│         │                  │                  │                  │        │
│  ┌──────┴──────┐    ┌──────┴──────┐    ┌──────┴──────┐    ┌──────┴──────┐ │
│  │     LR      │    │  GBDT+LR    │    │Deep Crossing│    │   DeepFM    │ │
│  │             │    │             │    │  YouTube DNN│    │     DCN     │ │
│  └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘ │
│                                                                            │
├─────────────────────────────── 特征表示 ───────────────────────────────────┤
│                                                                            │
│  [离散·稀疏]  →  [离散·自动]  →  [连续·稠密]  →  [结构化·混合]            │
│  One-Hot编码      决策路径编码     Embedding         显式+隐式              │
│                                                                            │
├─────────────────────────────── 核心能力 ───────────────────────────────────┤
│                                                                            │
│   人工设计    →   组合自动化   →   语义表示    →   端到端学习              │
│  经验依赖强       减少人工         可泛化            完全自动               │
│                                                                            │
├─────────────────────────────── 交叉建模 ───────────────────────────────────┤
│                                                                            │
│   手工交叉    →   路径编码     →   MLP隐式    →   FM显式+MLP隐式          │
│   O(n²)人工       树结构编码       黑盒学习        ⟨vi,vj⟩ + DNN          │
│                                                                            │
├─────────────────────────────── 演进主线 ───────────────────────────────────┤
│                                                                            │
│          ╔═══════════════════════════════════════════════════╗            │
│          ║  自动化程度 ↑ │ 表示能力 ↑ │ 端到端程度 ↑        ║            │
│          ╚═══════════════════════════════════════════════════╝            │
│                                                                            │
│                              ↓ 终极目标 ↓                                  │
│         ┌───────────────────────────────────────────────────┐             │
│         │        构建通用语义空间（User-Item-Context）       │             │
│         │  • 相似性可度量  • 关系可推理  • 组合效应可表达    │             │
│         └───────────────────────────────────────────────────┘             │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘

一句话总结：推荐系统的进化，就是让模型学会为特定任务自动构建最优"特征语言"的过程。
```

---

## 📈 技术发展时间线

```
2014         2016              2017           2018+
 │            │                 │              │
 ▼            ▼                 ▼              ▼
┌────┐    ┌───────────┐    ┌─────────┐    ┌───────────┐
│GBDT│    │Wide & Deep│    │ DeepFM  │    │ Transformer│
│+LR │    │Deep Cross │    │   DCN   │    │ 推荐模型   │
└────┘    │YouTube DNN│    │xDeepFM  │    │SASRec/BERT│
          └───────────┘    └─────────┘    └───────────┘
   │            │                │              │
   ▼            ▼                ▼              ▼
组合自动化   Embedding革命    混合架构成熟   注意力机制
```

---

## 🔑 关键要点（面试/复习用）

### 一、基础概念类

#### Q1: 为什么LR需要人工特征工程？
> LR只能学习线性叠加关系 $y=\sum w_i x_i$，无法捕捉特征间的组合效应，必须人工构造交叉特征。

#### Q2: 什么是特征交叉？为什么重要？
> **特征交叉**是将多个特征组合成新特征，捕捉它们的联合效应。
> - 例如：单独的"年龄"和"商品类别"无法表达"年轻人喜欢电竞产品"
> - 交叉特征 `age=young & category=gaming` 才能捕捉这种组合模式
> - 重要性：现实世界中大量规律是组合性的，非线性的

#### Q3: Embedding的三重价值？
> 1. **降维**：百万维→百维；2. **语义化**：向量距离承载相似性；3. **泛化**：未见过的组合也能通过向量运算推理（相似实体向量相近，知识可迁移）

#### Q4: Embedding是如何训练的？
> - **端到端训练**：作为模型参数，通过反向传播与其他参数一起优化
> - **预训练**：如Word2Vec思想，通过上下文共现关系预训练，再迁移到下游任务
> - **初始化**：通常随机初始化（Xavier/He），也可用预训练向量初始化

---

### 二、模型原理类

#### Q5: GBDT+LR的核心思想是什么？
> 用GBDT的决策路径（从根到叶）自动编码特征组合，每条路径对应一个逻辑合取，再将叶节点One-Hot化输入LR。

#### Q6: GBDT+LR为什么是两阶段？有什么问题？
> - **两阶段**：先训练GBDT得到叶节点编码，再训练LR
> - **问题**：GBDT的优化目标与最终CTR目标不一致，无法端到端联合优化
> - **后果**：特征表示不是为最终任务量身定制的

#### Q7: FM组件为什么高效？数学原理是什么？
> 原始二阶交叉需要 $O(n^2)$ 参数和计算：$\sum_{i}\sum_{j>i} w_{ij} x_i x_j$
>
> FM通过矩阵分解，用 $k$ 维向量 $v_i$ 表示每个特征：$w_{ij} \approx \langle v_i, v_j \rangle$
>
> 关键公式变换：
> $$\sum_{i}\sum_{j>i} \langle v_i, v_j \rangle x_i x_j = \frac{1}{2}\left[\left(\sum_i v_i x_i\right)^2 - \sum_i v_i^2 x_i^2\right]$$
>
> 复杂度从 $O(kn^2)$ 降至 $O(kn)$，且能处理稀疏特征的交叉

#### Q8: Wide & Deep vs DeepFM 的核心区别？
| 维度 | Wide & Deep | DeepFM |
|-----|-------------|--------|
| Wide部分 | 线性模型 + **手工交叉特征** | FM（**自动**二阶交叉） |
| 特征工程 | 仍需人工设计 | 完全自动 |
| Embedding | Wide和Deep**不共享** | FM和DNN**共享** |
| 端到端程度 | 部分 | 完全 |

#### Q9: DCN的Cross Network是怎么工作的？
> Cross Network通过显式的交叉层实现高阶特征交叉：
> $$x_{l+1} = x_0 \cdot x_l^T \cdot w_l + b_l + x_l$$
>
> - 每层增加一阶交叉，$L$ 层可建模 $L+1$ 阶交叉
> - 参数量仅 $O(d \cdot L)$，远小于MLP
> - 保持了显式交叉的可解释性

#### Q10: 为什么MLP不擅长学习低阶特征交叉？
> - MLP是"通用函数逼近器"，理论上能学任何函数
> - 但**全连接结构缺乏归纳偏置**，对于简单的二阶交叉也需要大量参数去"发现"
> - 类比：用微积分解 $1+1=2$，能解但大材小用且低效
> - 所以需要FM等结构**显式引导**低阶交叉的学习

---

### 三、核心概念辨析类

#### Q11: 什么是"记忆"与"泛化"？
| 能力 | 含义 | 类比 | 对应组件 |
|-----|------|-----|---------|
| **记忆** | 记住训练数据中的高频共现模式 | 背答案 | Wide/FM |
| **泛化** | 推理未见过的组合 | 学方法 | Deep/MLP |

> 例：训练集中"王者荣耀→游戏手柄"出现1000次，Wide直接记住这个规则（高权重）
> 
> 新游戏"原神"上线，Wide不认识，但Deep可通过Embedding相似性推理

#### Q12: 显式交叉 vs 隐式交叉？
| 类型 | 代表 | 特点 | 优势 | 劣势 |
|-----|------|------|-----|------|
| **显式** | FM, Cross Network | 有明确的交叉公式 | 可解释、参数高效 | 交叉形式固定 |
| **隐式** | MLP | 通过非线性变换隐式学习 | 灵活、可学任意模式 | 黑盒、参数多 |

> 最佳实践：显式处理低阶（重要且明确），隐式处理高阶（复杂且难以预设）

#### Q13: 为什么DeepFM要共享Embedding？
> 1. **一致性**：FM和DNN看到相同的特征语义表示
> 2. **参数效率**：避免两套Embedding的冗余
> 3. **联合优化**：低阶和高阶信号共同指导Embedding学习，表示更充分
> 4. **简化工程**：无需分别维护两套Embedding

---

### 四、实践应用类

#### Q14: 推荐系统的冷启动问题怎么解决？
> **冷启动类型**：
> - 用户冷启动：新用户无历史行为
> - 物品冷启动：新物品无曝光数据
>
> **Embedding相关方案**：
> - **边信息Embedding**：用用户属性（年龄、性别）或物品属性（类别、标签）的Embedding组合
> - **预训练迁移**：从其他数据源预训练Embedding
> - **元学习**：学习快速适应新用户/物品的能力
>
> ⚠️ 注意：纯ID Embedding无法解决冷启动，必须借助边信息

#### Q15: 特征交叉的阶数如何选择？
> - **一阶**：单特征的独立贡献（如热门物品bias）
> - **二阶**：最常见也最重要，大部分业务规律都是二元关系
> - **三阶及以上**：边际收益递减，且容易过拟合
>
> 实践建议：
> - 二阶用FM显式建模（必须有）
> - 高阶用MLP隐式学习（让模型自己发现）
> - 极少需要显式建模三阶以上交叉

#### Q16: 工业界如何选择推荐模型？
| 场景 | 推荐模型 | 理由 |
|-----|---------|------|
| 快速上线/资源有限 | LR + 精细特征工程 | 简单可控，效果可接受 |
| 标准CTR预估 | DeepFM / DCN | 自动化程度高，效果好 |
| 有丰富序列行为 | DIN / DIEN / SASRec | 能建模用户兴趣演化 |
| 超大规模稀疏特征 | 双塔模型 + ANN检索 | 召回效率高 |

#### Q17: Embedding维度如何选择？
> **经验公式**：$d = 6 \times (\text{类别数})^{1/4}$ 或直接取 $2^k$（如8, 16, 32, 64, 128）
>
> **权衡因素**：
> - 维度↑：表达能力↑，但参数量↑、过拟合风险↑
> - 维度↓：参数少，但可能欠拟合
>
> **实践建议**：
> - 高频特征（如用户ID）：较高维度（64-128）
> - 低频特征（如城市）：较低维度（8-32）
> - 通过实验调优，观察AUC变化

---

### 五、进阶思考类

#### Q18: 为什么Transformer能用于推荐系统？
> - **自注意力机制**：动态计算序列中任意两个位置的关联权重
> - **对比传统方法**：
>   - RNN：顺序处理，难以捕捉长距离依赖
>   - CNN：固定感受野，灵活性差
>   - Transformer：全局注意力，并行计算
> - **代表模型**：SASRec、BERT4Rec、Transformer4Rec

#### Q19: 推荐系统特征表示的未来趋势？
> 1. **多模态融合**：文本、图像、视频Embedding统一建模
> 2. **预训练大模型**：如GPT-based推荐，通用表示+任务微调
> 3. **图神经网络**：建模用户-物品交互图的高阶关系
> 4. **因果推断**：从相关性到因果性，提升推荐的可解释性
> 5. **实时个性化**：在线学习Embedding，快速适应用户兴趣变化

#### Q20: 一句话总结推荐系统特征表示的演进？
> **从人工定义离散符号，到模型自主学习连续语义向量**——本质是让机器学会为特定任务自动构建最优"特征语言"。

---

### 六、快速记忆卡片

| 问题 | 一句话答案 |
|-----|----------|
| LR为什么要人工交叉 | 只能线性叠加，无法自动组合 |
| GBDT+LR的本质 | 用树的路径自动做特征工程 |
| Embedding的本质 | 把离散ID映射到连续语义空间 |
| FM的优雅之处 | 向量内积建模交叉，$O(n^2)→O(kn)$ |
| Wide vs Deep | 记忆(背答案) vs 泛化(学方法) |
| DeepFM的创新 | FM+MLP共享Embedding，全自动端到端 |
| 显式vs隐式交叉 | 有公式vs黑盒学习，各有所长 |
| 冷启动怎么办 | 用边信息(属性)Embedding，不能只靠ID |

---

*最后更新：2026-01-07*
