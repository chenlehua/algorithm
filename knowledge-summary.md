# 推荐系统核心知识速查手册

> 🎯 **一句话总结**：推荐系统的进化 = 让机器学会自动构建"特征语言"的过程

---

## 🗺️ 知识地图

```
你在哪个阶段？

问题：如何让模型理解"年轻人喜欢电竞产品"？

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   方案1          方案2           方案3          方案4           │
│   人工造         树来帮          向量化         显+隐结合       │
│                                                                 │
│   ┌─────┐       ┌─────┐        ┌─────┐        ┌─────┐         │
│   │ LR  │  →    │GBDT │   →    │ DNN │   →    │Deep │         │
│   │     │       │+LR  │        │     │        │ FM  │         │
│   └─────┘       └─────┘        └─────┘        └─────┘         │
│                                                                 │
│   累死人         省点力          能泛化         最优解          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 📖 四个故事讲透四个阶段

### 故事1：特征工程师的996（LR时代）

**场景**：电商要预测用户是否点击广告

```
老板：小王，模型效果不好啊！
小王：我来加特征...

第1周：加了"年龄"、"性别"、"商品类别" → AUC提升0.01
第2周：加了"年龄×性别"、"年龄×商品" → AUC提升0.02
第3周：加了"年龄×性别×时段×商品×..." → 累死了

问题：
├── 100个特征 → 4950个二阶组合 → 人工设计不完
├── 大量组合没有数据 → 学不到
└── 业务变了 → 又要重新设计
```

**痛点口诀**：**爆·稀·滞**
- **爆**：组合爆炸，设计不完
- **稀**：数据稀疏，学不到
- **滞**：更新滞后，跟不上业务

---

### 故事2：GBDT来帮忙（树模型时代）

**场景**：让树模型自动发现特征组合

```
GBDT说：让我来帮你找规律！

训练后，树自动学到：
├── 树1：年龄<25 且 晚上 → 叶子A
├── 树2：收入>5000 且 男性 → 叶子B
└── ...

每个叶子 = 一个自动发现的规则
把叶子编号输入LR → 自动化的特征工程！
```

**GBDT工作原理**（射箭比喻）：

```
目标：射中10环

第1箭：射到6环（教练：往右上修正）
第2箭：修正后9环（教练：再微调）
第3箭：9.8环
第4箭：9.95环 ≈ 靶心！

GBDT = 每棵树修正前面的偏差，不断逼近目标
```

**进步**：不用手工设计特征组合了
**局限**：还是离散编码，"北京"和"上海"没有相似性

---

### 故事3：Embedding的魔法（深度学习时代）

**场景**：让相似的东西有相似的表示

```
传统方式：
  北京 = [1,0,0,0,0,...]  ← 百万维，只有一个1
  上海 = [0,1,0,0,0,...]  ← 和北京完全不相关！

Embedding方式：
  北京 = [0.8, 0.2, 0.5, ...]  ← 只有几十维
  上海 = [0.7, 0.3, 0.4, ...]  ← 和北京很接近！
  
神奇之处：
  ● 北京 和 ● 上海 在向量空间中离得很近
  ● 电竞鼠标 和 ● 机械键盘 也离得很近
  
  即使"深圳+电竞鼠标"从未出现过，
  也能推理出：深圳和北京相似，北京喜欢电竞，所以深圳可能也喜欢
```

**三重价值**：
| 价值 | 说明 | 比喻 |
|-----|------|------|
| 降维 | 百万维→百维 | 压缩包 |
| 语义 | 相似的东西向量相近 | 地图上相邻 |
| 泛化 | 没见过也能推理 | 举一反三 |

---

### 故事4：记忆+泛化（混合架构时代）

**场景**：背答案和学方法，我全都要！

```
考试策略：

学霸A（只背答案 = Wide）：
  ├── 见过的题：秒杀 ✓
  └── 没见过的题：懵逼 ✗

学霸B（只学方法 = Deep）：
  ├── 见过的题：也能做，但没那么快
  └── 没见过的题：能推理 ✓

学神（背答案+学方法 = Wide & Deep / DeepFM）：
  ├── 见过的题：秒杀 ✓
  └── 没见过的题：也能推理 ✓
```

**实际例子**：

```
训练数据：
  "王者荣耀用户" → 80%点击"游戏手柄"
  
Wide部分（记忆）：
  直接记住：王者荣耀+游戏手柄 = 高权重
  
Deep部分（泛化）：
  学到：王者荣耀的Embedding ≈ 原神的Embedding
  
新游戏"原神"上线：
  Wide：不认识，权重=0 ✗
  Deep：原神≈王者荣耀，推理出也喜欢游戏手柄 ✓
  
结合：高频模式靠记忆，长尾需求靠泛化
```

---

## 🧠 核心概念速查

### 概念1：特征交叉

```
问题：单独看"年龄=25"和"商品=电竞鼠标"没用
答案：组合起来"年轻人+电竞鼠标" = 强信号

类比：
  单独的"花生"和"酱" → 普通食材
  "花生+酱" = 花生酱 → 美味！
```

### 概念2：离散 vs 连续

```
离散（One-Hot）：
  北京 = [1,0,0,0,0,0,0,0,0,0,...] 
  上海 = [0,1,0,0,0,0,0,0,0,0,...]
  → 两个城市"距离"一样远，没有相似性

连续（Embedding）：
  北京 = [0.8, 0.2, 0.5]
  上海 = [0.7, 0.3, 0.4]
  深圳 = [0.75, 0.25, 0.45]
  → 一线城市聚在一起，有相似性
```

### 概念3：显式 vs 隐式交叉

```
显式（FM）：
  我明确告诉你：交叉强度 = 向量内积
  计算：<v年龄, v电竞> = 0.8
  可解释 ✓

隐式（MLP）：
  扔进神经网络，让它自己学
  输出：0.8
  黑盒 ✗

最佳实践：低阶用显式（重要+明确），高阶用隐式（复杂+未知）
```

### 概念4：端到端 vs 两阶段

```
两阶段（GBDT+LR）：
  先训练GBDT → 再训练LR → 各干各的，目标可能不一致

端到端（DeepFM）：
  FM和DNN一起训练 → 共同优化同一个目标 → 更协调
```

---

## 🔧 GBDT核心原理

### 一句话

```
串行训练多棵树，每棵专门修正前面的错误，最后全部加起来
```

### 生动比喻

```
【学生改错题】

目标：考100分

第1次：考了60分，错了3道
第2次：只学错题 → 80分
第3次：继续攻克 → 93分
第4次：→ 97分
...
关键：每次只关注"还没学好的"
```

### 训练过程

```
【班主任预测成绩】

5个学生，真实成绩：95, 88, 75, 65, 55

第0步：先猜平均分 = 75.6

第1步：算偏差
  小明：95-75.6 = +19.4（猜低了）
  小强：55-75.6 = -20.6（猜高了）

第2步：训练树1，学习"谁猜偏了"
  发现规律：作业>80%的，猜低了15分

第3步：更新预测（只采纳一半，防止过猛）
  小明：75.6 + 0.5×15 = 83.1

第4步：再算偏差，训练树2...

最终：树1 + 树2 + 树3 + ... = 精准预测
```

### 树怎么"长"

```
【班长分组】

目标：让每组内的人"偏差相近"

尝试1：按作业>80%分组 → 组内方差=79（较好）
尝试2：按睡眠>7h分组 → 组内方差=408（较差）

选择：方差最小的分法！

叶子节点值 = 组内偏差的平均值
```

---

## 📊 模型选择速查

| 场景 | 推荐方案 | 理由 |
|-----|---------|------|
| 快速上线 | LR + 精细特征工程 | 简单可控 |
| 标准CTR | DeepFM / DCN | 自动化高，效果好 |
| 有用户序列 | DIN / SASRec | 能捕捉兴趣变化 |
| 超大规模 | 双塔 + 近似检索 | 效率高 |

---

## 🎓 记忆口诀

### 四阶段

```
人工交叉累成狗（LR）
树模型来帮帮手（GBDT+LR）
Embedding语义有（Deep Learning）
显隐结合全都有（DeepFM）
```

### GBDT

```
猜一猜（初始化平均值）
看偏差（算残差）
树来帮（拟合残差）
加起来（累加预测）
再重复（迭代优化）
```

### 核心对比

```
Wide vs Deep = 背答案 vs 学方法
显式 vs 隐式 = 有公式 vs 黑盒
离散 vs 连续 = 无相似 vs 可泛化
端到端 vs 两阶段 = 协调 vs 割裂
```

---

## ❓ 高频面试题

### Q：为什么LR要人工交叉？
> LR只会"加"，不会"乘"。要表达"年轻×电竞"，必须人工构造。

### Q：GBDT+LR的本质？
> 用树的路径自动做特征工程，每个叶子=一条规则。

### Q：Embedding凭什么能泛化？
> 相似的东西向量相近，没见过的也能通过"邻居"推理。

### Q：Wide & Deep为什么要结合？
> Wide背高频答案，Deep学长尾方法，互补。

### Q：FM为什么比MLP高效？
> FM直接用向量内积算交叉，MLP要用大量参数去"发现"交叉。

### Q：冷启动怎么办？
> 不能只靠ID，要用属性（年龄、类别等）的Embedding。

---

## 🖼️ 一张图总结

```
┌────────────────────────────────────────────────────────────────┐
│                     推荐系统特征演进                            │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  人工时代        树模型          深度学习        混合架构       │
│  ─────────────────────────────────────────────────────────→   │
│                                                                │
│  [累死人]  →   [省点力]   →   [能泛化]   →   [最优解]         │
│                                                                │
│   手工造         自动挖          向量化          显+隐          │
│   交叉特征       特征组合        语义表示        一起学         │
│                                                                │
├────────────────────────────────────────────────────────────────┤
│  核心进步：                                                     │
│  • 自动化程度 ↑（从人工到自动）                                 │
│  • 表示能力 ↑（从离散到连续）                                   │
│  • 端到端程度 ↑（从割裂到统一）                                 │
├────────────────────────────────────────────────────────────────┤
│  终极目标：构建"用户-物品-上下文"的通用语义空间                 │
│  • 相似的东西离得近                                             │
│  • 没见过的也能推理                                             │
│  • 组合效应自动捕捉                                             │
└────────────────────────────────────────────────────────────────┘
```

---

## 📚 文档索引

| 文档 | 内容 | 适合场景 |
|-----|------|---------|
| [本文档](./knowledge-summary.md) | 核心知识速查 | 快速复习、面试准备 |
| [特征演进详解](./recommendation-feature-evolution.md) | 完整技术细节 | 深入学习 |
| [GBDT详解](./gbdt-explained.md) | GBDT原理与训练 | 理解树模型 |

---

*一句话带走：推荐系统进化 = 让机器学会自己发明"特征语言"*

---

*最后更新：2026-01-07*
