# 连接主义：感知机与浅层网络（知识总结）

> 🎯 **核心命题**：深度学习的本质是"表示设计"——让模型自动学习如何把原始数据转换成解决问题所需的"通用语言"

---

## 🗺️ 知识地图

```
从手工到自动，从记忆到泛化

┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   LR          GBDT+LR       MLP          Wide&Deep    DeepFM   │
│   手工交叉     树来帮忙       端到端        记忆+泛化     全自动   │
│                                                                 │
│   ┌─────┐    ┌─────┐      ┌─────┐      ┌─────┐      ┌─────┐   │
│   │线性 │ →  │分阶段│  →   │一体化│  →   │双路 │  →   │共享 │   │
│   │边界 │    │割裂 │      │训练 │      │分离 │      │Emb │   │
│   └─────┘    └─────┘      └─────┘      └─────┘      └─────┘   │
│                                                                 │
│   只会加        自动挖        能非线性      Wide要人工    FM自动   │
│   不会乘        但割裂        但盲目        Deep能泛化    全搞定   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 📖 核心概念详解

### 1️⃣ 协同过滤的本质

**核心思想**：人以群分，物以类聚

```
【旅行推荐的例子】

单独看"长途目的地"：用户搜索上海→夏威夷
  → 意图模糊：度假？蜜月？出差？

单独看"家庭出行"：选择3人出行、浏览亲子酒店
  → 意图宽泛：周边游？家庭聚会？

组合起来"长途 + 家庭"：
  → 精准画像："年度重磅家庭度假"
  → 商业价值：高客单价、提前预订

结论：特征组合（交叉）才是关键！
```

---

### 2️⃣ LR的困境：只会"加"，不会"乘"

```
【LR的能力】

LR模型：预测 = w1×特征1 + w2×特征2 + ...

就像做菜只会放盐和放糖，但不会让它们产生化学反应

问题：
  单独的"年龄=25" → 知道是年轻人
  单独的"商品=电竞鼠标" → 知道是电竞产品
  但 LR 无法自动理解"年轻人 × 电竞鼠标" = 强组合信号！

解决方案（传统）：
  特征工程师手工构造：age_young_AND_category_gaming = 1
  → 累死人！100个特征就有4950个二阶组合
```

**LR = 一个没有隐藏层的神经网络 = 最简单的神经网络**

---

### 3️⃣ GBDT+LR：让树来帮忙做特征工程

```
【两阶段方案】

阶段1：GBDT 自动挖掘特征组合
  用户A的数据 → 走过决策树 → 落入叶子节点
  
  树1: age<25? → Yes → income>5000? → Yes → 叶子3
  编码: [0, 0, 1, 0]  # 第3位为1
  
  含义：叶子3 = "年轻且高收入"这个规则

阶段2：LR 对叶子编码进行加权预测
  把所有树的叶子编码拼接 → 输入LR → 输出预测

优点：不用手工设计交叉特征了
缺点：两阶段割裂，GBDT和LR各干各的，目标不一致
```

---

### 4️⃣ MLP的端到端革命

**核心创新**：加入隐藏层，获得非线性能力

```
【LR vs MLP】

LR（无隐藏层）：
  输入 → 直接加权求和 → 输出
  只能画一条直线分割数据

MLP（有隐藏层）：
  输入 → 隐藏层（非线性变换）→ 输出
  可以画任意曲线分割数据！

【隐藏层在干什么？】

每个隐藏层神经元就像一个"小型LR"：
  神经元1：学会识别 "年龄<20 且 城市=北京"
  神经元2：学会识别 "性别=女 且 喜欢购物"
  神经元3：学会识别 "收入>10000 且 有车"
  ...

多层叠加 → 学习任意复杂的组合模式！
```

---

### 5️⃣ MLP如何学习：前向传播 + 反向传播

```
【学习的完整周期】

1. 初始状态：权重是随机的（空白大脑）

2. 前向传播：输入数据，产生预测
   图片 → 网络 → 预测"狗" 
   但真实是"猫" → 产生误差！

3. 反向传播：追溯责任，分配贡献
   误差像回声一样逐层向后传递
   每个权重根据自己的"贡献度"被追责

4. 梯度下降：微调权重
   新权重 = 旧权重 - 学习率 × 梯度
   
5. 循环往复：千万次迭代
   最终权重被"雕刻"成能准确预测的形状
```

**比喻**：
- LR的优化 = 在一个碗里找最低点（凸函数，一定能找到）
- MLP的优化 = 在崎岖山脉里找最低谷（非凸函数，可能陷入局部最优）

---

### 6️⃣ Embedding：解决"海量ID"的参数爆炸问题

```
【问题】

假设有100万个商品ID，MLP第一层有512个神经元
参数量 = 100万 × 512 = 5亿参数！
→ 内存装不下，计算跑不动

【Embedding的解决方案】

不用One-Hot（100万维），而是给每个ID一个小向量（如64维）

工作方式：
  商品ID = 9527 → 查表 → 取出第9527行 → 得到64维向量
  
参数量 = 100万 × 64 = 6400万 ← 减少8倍！

方案           输入维度        效果
One-Hot       100万维         ❌ 灾难
Embedding     64维            ✅ 可行
```

---

### 7️⃣ Embedding的语义：监督信号的"雕刻"

```
【Embedding为什么有语义？】

不是先验知识，而是训练数据"雕刻"出来的！

例：CTR预估场景

训练数据中的模式：
  • 看过"可口可乐"的用户 → 也爱点击"百事可乐" ✓
  • 看过"可口可乐"的用户 → 很少点击"米其林轮胎" ✗

反向传播的指令：
  • "拉近可口可乐和百事可乐的向量！"
  • "推远可口可乐和轮胎的向量！"

结果：
  可口可乐 ● ────● 百事可乐  （向量相近）
  
  米其林轮胎 ●                （向量很远）

泛化能力：
  新品"元气森林"上线 → 向量会被训练得接近"可口可乐"
  → 即使没见过的组合，也能通过向量相似性推理！
```

---

### 8️⃣ MLP的特征交叉：隐式、逐层累积

```
【第一层：不是真正的交叉】

h1 = 激活函数(w1×x1 + w2×x2 + w3×x3 + ...)

虽然多个特征被组合，但本质是"线性投影+非线性变换"
每个权重是独立的，没有显式建模特征之间的交互

【第二层开始：真正的交叉能力】

第二层的输入 = 第一层的输出（已经做过组合变换的信号）
→ 形成"组合的组合"

网络层级      学习内容              对应阶数
第1层        原始特征的非线性组合    一阶
第2层        第一层的再组合          二阶及以上
第L层        L-1层的再组合           高阶

【结论】
真正的特征交叉从第二层才开始！
层数越深，能学习的交叉阶数越高
```

---

### 9️⃣ MLP的"学习盲区"：对低阶交叉不够重视

```
【问题】

MLP是"万能逼近器"，理论上能学任何函数
但它对所有可能的交叉"一视同仁"，没有偏好

这种"无偏好"可能导致：
  • 浪费容量探索无意义的高阶组合（如 用户城市×商品批次×当前小时）
  • 对真正重要的低阶交叉（如 用户ID×商品ID）"投入不足"

【具体例子】

user_id × item_id 是协同过滤的核心！

但对于MLP：
  • 热门组合（数据多）：能通过"暴力"学习捕捉到
  • 长尾组合（数据少）：信号微弱，可能被淹没在噪声中

【启发】
能不能设计一个结构，专门高效学习重要的低阶交叉？
→ 这就是 Wide & Deep 和 DeepFM 的动机！
```

---

### 🔟 Wide & Deep：记忆 + 泛化

```
【核心思想】

推荐系统需要同时具备两种能力：
  记忆：记住历史中频繁共现的模式（如 张三爱买手机A）
  泛化：推理从未见过的组合（如 年轻男性可能喜欢游戏本）

【架构】

        最终预测
           ↑
    ┌──────┴──────┐
    │             │
  Wide          Deep
 (记忆)        (泛化)
    │             │
手工交叉      Embedding
 特征          + MLP

【分工】

Wide部分：
  输入：手工设计的交叉特征（如 user_id×item_id）
  能力：直接给高频组合高权重 → "背答案"
  
Deep部分：
  输入：Embedding向量
  能力：通过MLP学习高阶模式 → "学方法"

【局限】
Wide部分还是要人工设计交叉特征！
Wide和Deep用两套不同的输入，没有共享
```

---

### 1️⃣1️⃣ DeepFM：真正的端到端

```
【核心创新】

创新1：用FM替代Wide
  FM能自动学习所有二阶交叉！
  不再需要人工设计交叉特征

创新2：共享Embedding
  FM和Deep共用同一套Embedding
  一份数据，两路梯度，学习更充分

【架构】

           最终预测
              ↑
       ┌──────┴──────┐
       │             │
      FM           Deep
   (二阶交叉)      (高阶交叉)
       │             │
       └──────┬──────┘
              │
        共享Embedding  ← 精髓！
              │
         原始特征

【FM如何工作】

每个特征有一个向量：
  v_用户A = [0.8, 0.2, 0.1]
  v_商品X = [0.9, 0.1, 0.2]

交叉强度 = 向量内积：
  用户A × 商品X = 0.8×0.9 + 0.2×0.1 + 0.1×0.2 = 0.76

FM输出 = 所有二阶交叉的内积之和
```

---

### 1️⃣2️⃣ DeepFM vs Wide&Deep

| 对比维度 | Wide & Deep | DeepFM |
|---------|-------------|--------|
| Wide/FM部分 | 需要人工设计交叉 | FM自动学习二阶交叉 |
| Embedding | Wide和Deep各有一套 | **共享同一套** |
| 端到端程度 | 部分（Wide要人工） | **完全端到端** |
| 维护成本 | 高（要维护交叉特征） | 低（全自动） |

---

## 🎓 深度学习的本质

### 核心洞察

```
深度学习 = 面向场景需求的"表示设计"

不同场景需要不同的表示函数：

场景                    需要的表示                解决方案
──────────────────────────────────────────────────────────
二阶交叉很重要          显式的A×B关系            FM（向量内积）
探索复杂高阶模式        灵活的非线性组合          MLP（深度网络）
序列化特征（点击历史）  时序依赖、兴趣演化        Transformer/RNN
图结构特征（社交网络）  邻居信息聚合              GNN

核心任务：从工具箱中选择或创造最适合业务的"表示函数"
```

### 学习的工程化实现

```
联结主义的信念：知识可以"生长"在网络权重中

实现方式：
1. 前向传播：输入数据，产生预测，计算误差
2. 反向传播：追溯每个权重的"责任"（梯度）
3. 梯度下降：微调权重，减小误差
4. 循环往复：亿万次迭代，权重被"雕刻"成最优形状

最终成果：学会把原始数据转换成"通用语言"（向量表示）
```

---

## 📝 记忆口诀

### 模型演进

```
LR只会加，交叉累死人（人工特征工程）
树来帮帮忙，两阶段割裂（GBDT+LR）
MLP端到端，但对低阶盲（隐式交叉）
Wide背答案，Deep学方法（Wide & Deep）
DeepFM共享，全自动搞定（FM + 共享Embedding）
```

### 核心概念

```
Embedding三件事：降维、语义、泛化
MLP特征交叉：第二层才开始有
Wide vs Deep：记忆 vs 泛化
DeepFM精髓：FM自动 + Embedding共享
```

---

## ❓ 高频面试题

### Q1: 为什么LR需要人工特征交叉？
> LR只能学习特征的线性叠加，无法自动发现"年轻×电竞"这种组合效应。

### Q2: MLP的特征交叉从哪一层开始？
> 从第二层开始。第一层只是"线性投影+非线性变换"，第二层才是"组合的组合"。

### Q3: Embedding的语义是怎么来的？
> 不是先验知识，而是监督信号在训练中"雕刻"出来的。相似行为的ID会被拉近，不相似的会被推远。

### Q4: Wide & Deep的局限是什么？
> Wide部分还是要人工设计交叉特征；Wide和Deep用两套输入，没有共享Embedding。

### Q5: DeepFM相比Wide & Deep的核心改进？
> 1. 用FM替代Wide，自动学习二阶交叉；2. FM和Deep共享Embedding，一份数据两路梯度。

### Q6: 深度学习的本质是什么？
> 面向场景需求的"表示设计"——让模型自动学习把原始数据转换成解决问题所需的向量表示。

---

## 🖼️ 知识全景图

```
┌────────────────────────────────────────────────────────────────┐
│                    感知机与浅层网络全景                          │
├────────────────────────────────────────────────────────────────┤
│                                                                │
│  问题起点：如何让模型理解"特征组合"的效应？                      │
│                                                                │
├─────────────── 解决方案演进 ───────────────────────────────────┤
│                                                                │
│  LR          GBDT+LR        MLP          Wide&Deep    DeepFM   │
│  ────────────────────────────────────────────────────────────  │
│  人工交叉     自动挖掘        端到端        记忆+泛化    全自动   │
│  累死人       但割裂          但盲目        Wide要人工   FM搞定  │
│                                                                │
├─────────────── 核心技术 ─────────────────────────────────────┤
│                                                                │
│  Embedding: 解决高维稀疏 → 低维稠密，有语义，能泛化              │
│  隐藏层: 获得非线性能力，学习任意复杂模式                        │
│  反向传播: 追溯责任，梯度下降，迭代优化                          │
│  FM: 自动学习所有二阶交叉，向量内积表示交叉强度                  │
│                                                                │
├─────────────── 本质洞察 ─────────────────────────────────────┤
│                                                                │
│  深度学习 = 表示设计                                            │
│  核心任务：选择或创造最适合业务的"表示函数"                      │
│  最终成果：把原始数据转换成解决问题的"通用语言"                  │
│                                                                │
└────────────────────────────────────────────────────────────────┘
```

---

*最后更新：2026-01-07*
